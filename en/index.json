[{"content":"Introduction In system engineering, it’s tempting to treat “technically correct” as the finish line.\nBut I’ve learned the hard way that correctness can still create a bad customer experience—especially when your software is sitting between messy reality (hardware, firmware, drivers) and the person who just wants a clear signal: Is something wrong, and do I need to act now?\nThis is the story of one uncomfortable moment that taught me a simple heuristic: sometimes, software has to absorb hardware noise.\nThe Problem: A Cascade of False Alarms We run an SSD monitoring daemon that reports wear-out metrics to customers. One day, a customer escalated a critical issue: their fleet was suddenly triggering SSD wear-out alarms at scale.\nThe timing couldn’t have been worse. Their release was measured in days, not weeks. And an “SSD health crisis” alert storm is the kind of thing that freezes decision-making—even if the systems are actually fine.\nMy first reaction was the classic engineer reflex: assume the bug is ours.\nSo I reviewed the code line by line. And frustratingly, it looked solid. We were reading the OS signals and reporting them exactly as designed.\nThen we found the uncomfortable truth: the signal itself was flaky.\nUnderneath our daemon, a specific SSD firmware could produce false positives at a lower layer. And our software—being “correct”—was faithfully amplifying that flakiness into customer-facing alarms.\nThat created a dilemma I didn’t love, but couldn’t avoid:\nThe implementation was technically correct, but it was failing the customer by producing noise instead of actionable information. This customer was the first to report the incident, so we didn’t have a playbook. The hardware vendor’s RMA process would take weeks. The customer’s release was in days. The Solution: A Heuristic Filter We needed something fast, safe, and honest.\nSo I implemented a heuristic filter—a debouncing approach.\nIn plain English: instead of alarming the moment we see a single “wear” signal, we require the signal to persist across multiple reads (or across a short time window). If the signal is transient and disappears on the next check, we treat it as noise. If it stays, we treat it as real.\nThe key tradeoff is latency. Our daemon checks the signal every few seconds, so “confirming” persistence can delay an alert by seconds (sometimes minutes, depending on the window).\nFor SSD health monitoring, that slight delay is a good deal. Wear-out isn’t a millisecond-scale emergency; it’s a trend. In this context, it’s better to report a slightly delayed, trustworthy alert than to flood users with spikes that they can’t act on.\nI aligned with my manager on the framing:\nThis is a pragmatic patch to unblock the release. We’re not hiding the problem—we’re dampening noise while we drive a permanent vendor fix.\nThe Result: Unblocked and Learned The patch deployed smoothly. The false alarms stopped immediately. The customer shipped on schedule.\nThat said, the deeper issue was (and still is) a mystery. We still hadn’t identified the culprit stage in production that led to the flaky behavior. I asked our drive team to continue the follow-up and push for a root-cause diagnosis.\nThe bigger lesson for me was not about SSDs—it was about what “correctness” means in the real world:\n“Technically correct” doesn’t automatically mean “works for the customer.”\nIf our job is to help customers operate systems confidently, then our job isn’t just to pass signals through. It’s to translate noisy reality into something stable, explainable, and actionable.\nHave you ever had to choose pragmatism over perfection to protect a customer experience? I’d love to hear what you did—and what you learned.\n","permalink":"https://nikeasyanzi.github.io/en/posts/when-software-must-absorb-hardware-noise/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn system engineering, it’s tempting to treat “technically correct” as the finish line.\u003c/p\u003e\n\u003cp\u003eBut I’ve learned the hard way that correctness can still create a bad customer experience—especially when your software is sitting between messy reality (hardware, firmware, drivers) and the person who just wants a clear signal: \u003cem\u003eIs something wrong, and do I need to act now?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThis is the story of one uncomfortable moment that taught me a simple heuristic: sometimes, software has to absorb hardware noise.\u003c/p\u003e","title":"The False Alarm Heuristic: When Software Must Absorb Hardware Noise"},{"content":"Observability source\nObservability is the practice of instrumenting a system to collect data that allows you to understand its internal state from the outside.\nIn modern complex environments like microservices, we need layered observability help us to tell the status from our business to the infrastructure underneath. Observability gives you the power to explore and debug problems you\u0026rsquo;ve never seen before (\u0026ldquo;unknown unknowns\u0026rdquo;) by asking arbitrary questions about your system\u0026rsquo;s behavior.\nThe pyramid clear depict a common business story from finding a service is down to IT need to deep dive to investigate the incident.\nTo achieve this, applications must be instrumented to emit telemetry—data about their performance and behavior. This data is typically categorized into three foundational pillars.\nThe Three Pillars of Observability Source\nWhile metrics, logs, and traces are often called the \u0026ldquo;three pillars,\u0026rdquo; their true power is unlocked when they are correlated, allowing you to move seamlessly between them to diagnose issues.\n1. Metrics: The \u0026ldquo;What\u0026rdquo; Metrics are aggregated, numerical data captured over a period of time. They are excellent for understanding the overall health of a system at a glance and for setting up alerts. ex. CPU usage、API response time\nWhat they answer: \u0026ldquo;What is the CPU usage?\u0026rdquo;, \u0026ldquo;What is the error rate of my API?\u0026rdquo;, \u0026ldquo;How many requests per second are we handling?\u0026rdquo;\nKey Concepts:\nDimensionality (維度 ): Metrics are more powerful when they have dimensions (also called labels or tags), which are key-value pairs that add context. For example, instead of just http_requests_total, you could have http_requests_total{method=\u0026quot;POST\u0026quot;, status=\u0026quot;500\u0026quot;}.\nCardinality (基數) : This refers to the number of unique combinations of dimensions. High cardinality (e.g., using a unique userID as a dimension) can be challenging for monitoring systems to store and query efficiently. As the number of time series grows, queries become more computationally expensive. Additionally, any significant event, e.g., a code release involving immutable infrastructure, will result in a flood of simultaneous writes to the database.\nTypical Tool: Prometheus is a leading open-source tool for collecting and storing time-series metric data.\n2. Logs: The \u0026ldquo;Why\u0026rdquo; A log is a timestamped, immutable record of a discrete event that occurred at a specific time. Logs provide the most detailed, granular context about what was happening inside an application or system. For instance, system error message or exception event\nWhat they answer: \u0026ldquo;Why did this specific request fail?\u0026rdquo;, \u0026ldquo;What was the exact error message?\u0026rdquo;, \u0026ldquo;What sequence of events led to this crash?\u0026rdquo;\nTypical Tool: Grafana Loki is a log aggregation system designed to be cost-effective and easy to operate, especially when integrated with other tools like Prometheus.\n3. Traces: The \u0026ldquo;Where\u0026rdquo; A trace represents the end-to-end journey of a single request as it moves through all the different services in a distributed system. Spans track specific operations that a request makes, painting a picture of what happened during the time in which that operation was executed.\nex. 在 iThome 使用 Facebook 的 SSO（Single Sign-On），一個單純的登入 “Request”，可能就橫跨了 iThome 的 Backend Service、Facebook 的 SSO Backend、iThome 的 Redis，這些衍生出來的 Requests，跟最初的登入 Request 結合起來呈現的歷程就稱為 Trace\nWhat they answer: \u0026ldquo;Where is the latency in this user\u0026rsquo;s request?\u0026rdquo;, \u0026ldquo;Which downstream service is causing the bottleneck?\u0026rdquo;, \u0026ldquo;What is the full path of this API call through our microservices?\u0026rdquo;\nTypical Tool: Grafana Tempo is a distributed tracing backend that can store and retrieve traces from various sources.\nEach unit of work within a trace is called a span.\nA large number of spans within a single trace: This would indicate a request that traverses many different services or performs a significant number of operations. While not inherently a problem, a trace with a very large number of spans could imply a highly complex interaction or potentially an inefficient design if too many microservices are involved for a simple request. It might make the visual representation of the trace more dense, but the primary purpose of traces is to map out these complex journeys.\nA span with a long duration (i.e., a \u0026ldquo;high\u0026rdquo; amount of time spent in that span): This is a direct and crucial insight that traces provide. If a particular span has a long duration, it means that the specific operation or service represented by that span is taking a significant amount of time to complete. This is exactly what traces are designed to help identify:\n\u0026ldquo;Where is the latency in this user\u0026rsquo;s request?\u0026rdquo; \u0026ldquo;Which downstream service is causing the bottleneck?\u0026rdquo; Therefore, a \u0026ldquo;high span\u0026rdquo; in terms of duration would pinpoint a performance bottleneck within your system, allowing you to investigate that specific operation or service further, potentially by jumping to its logs to find the root cause.\nBringing It All Together with Visualization The ultimate goal is to use these pillars together. A typical workflow might be:\nAn alert fires from a metric (e.g., latency is too high). You look at a trace for a slow request to see which specific service is the bottleneck. You jump from that trace to the logs of that service at that exact time to find the root cause error message. Grafana is the de-facto tool for creating dashboards that visualize and correlate data from all three pillars in a single pane of glass.\nEnabling Observability with OpenTelemetry Manually instrumenting code to produce all this data can be complex. OpenTelemetry is a vendor-neutral, open-source observability framework. It provides a standardized set of APIs, libraries, and agents to collect and export telemetry data (metrics, logs, and traces) from your applications, freeing you from being locked into a single vendor\u0026rsquo;s ecosystem.\nApplying Observability: Measuring Service Reliability differentiating-between-slo-vs-sla-vs-sli\nOnce you have rich telemetry data, you can move beyond just fixing bugs and start measuring what matters to your users and the business. This is done using a framework of SLIs, SLOs, and SLAs.\n1. Service Level Indicators (SLIs) An SLI is a direct, quantifiable measure of a service\u0026rsquo;s performance. It is the raw data you get from your observability tools.\nExample: The percentage of successful HTTP requests, or the latency of 95% of requests. 2. Service Level Objectives (SLOs) An SLO is the internal target you set for an SLI over a period of time. It is the goal your engineering team strives to meet.\nExample: 99.9% of API requests will be successful over a 30-day period. 3. Service Level Agreements (SLAs) An SLA is a formal, often contractual, commitment to a customer that defines the consequences if SLOs are not met (e.g., financial penalties or service credits). SLAs are typically less strict than internal SLOs to provide a buffer.\nExample: A promise of 99.5% uptime per month, with a 10% bill credit if breached. This framework connects the technical data from your observability platform directly to business and customer satisfaction goals. # Observability Observability lets you understand a system from the outside by ask questions about that system without knowing its inner workings.\nReference Observability primer\nObservability at Twitter: technical overview, part I\nObservability at Twitter: technical overview, part II [Observability 的過去與現在] https://ithelp.ithome.com.tw/m/articles/10319113\nWhy Your Observability Strategy Needs High Cardinality Data\nUnderstanding High Cardinality in Observability\n","permalink":"https://nikeasyanzi.github.io/en/posts/observability/","summary":"\u003ch1 id=\"observability\"\u003eObservability\u003c/h1\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i.imgur.com/FEPLgvU.png\"\u003e\n\u003ca href=\"https://developer.ibm.com/articles/observability-insights-and-automation/\"\u003esource\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eObservability\u003c/strong\u003e is the practice of instrumenting a system to collect data that allows you to understand its internal state from the outside.\u003c/p\u003e\n\u003cp\u003eIn modern complex environments like microservices, we need layered observability help us to tell the status from our business to the infrastructure underneath. Observability gives you the power to explore and debug problems you\u0026rsquo;ve never seen before (\u0026ldquo;unknown unknowns\u0026rdquo;) by asking arbitrary questions about your system\u0026rsquo;s behavior.\u003c/p\u003e","title":"Observability"},{"content":"Your English content goes here.\n","permalink":"https://nikeasyanzi.github.io/en/english-post/","summary":"\u003cp\u003eYour English content goes here.\u003c/p\u003e","title":"My English Post"},{"content":"Introduction In this article, we demonstrate how to effectively collecting logs with Alloy and Loki. With that, telemetry signals from applications, infrastructures are exported to on place for future telemetry.\nIn the lab exercise, we try to setup a Nginx service along with Loki, Alloy, and observe the system log on Grafana.\nConfiguration deep dive Grafana Alloy configuration A configuration file config.alloy is used, that contains logs to collect and where to forward them to.\nLoki Configuration Grafana Loki requires a configuration file to define how it should run. Within the loki-fundamentals directory, you will find a file called loki-config.yaml.\nGrafana Loki Data source This is used by Grafana to connect to Loki and query the logs. Grafana has multiple ways to define a data source;\nDirect: This is where you define the data source in the Grafana UI. Provisioning: This is where you define the data source in a configuration file and have Grafana automatically create the data source. API: This is where you use the Grafana API to create the data source. In the following lab part, we are using the provisioning method. We have defined the data source in this portion of the docker-compose.yml file:\nLab exercise The full configuration and docker-compose file can be found here In the repo, config.alloy is the configuration file for alloy. loki-config.yam is the configuration file for loki. grafana-dashboard.json grafana-datasources.yml defining the datasource for grafana grafana-default.yaml grafana-data is the folder for grafana database\nStarting the service docker-compose -f ./docker-compose.yml up -d Next, we check the service status.\nnginx http://10.122.168.81:8082/stub_status loki http://localhost:3100/metrics alloy http://localhost:12345/ When checking the alloy status, we see the loki components are up and running. We can also navigate the Alloy UI at http://localhost:12345/graph to see the configuration file config.alloyvisually. The configuration file defines logs to collect and where to forward them to.\ndiscovery.docker: This component queries the metadata of the Docker environment via the Docker socket and discovers new containers, as well as providing metadata about the containers.\ndiscovery.relabel: This component converts a metadata (__meta_docker_container_name) label into a Loki label (container).\nloki.source.docker: This component collects logs from the discovered containers and forwards them to the next component. It requests the metadata from the discovery.docker component and applies the relabeling rules from the discovery.relabel component.\nloki.process: This component provides stages for log transformation and extraction. In this case it adds a static label env=production to all logs.\nloki.write: This component writes the logs to Loki. It forwards the logs to the Loki endpoint http://loki:3100/loki/api/v1/push.\nNext, we configure Grafana to visualize the data collected by loki. Click Save\u0026amp;test to check the connectivity. Check logs of our services Once the service is up and running, we can navigate to http://localhost:3000/drilldown. Select Logs. You should see the Grafana Logs Drilldown page.\nWe can see Alloy is able to tail the Nginx log, also other container\u0026rsquo;s logs are also collected. Query logs To manually query Loki to ask more advanced questions about the logs, we use Grafana Explore. Open a browser and navigate to http://localhost:3000 to open Grafana. From the Grafana main menu, click the Explore icon (1) to open the Explore tab. Click Code to work in Code mode and type {container=\u0026ldquo;nginx\u0026rdquo;} Also here , we keep restart the Nginx service every 30s.\n#!/bin/bash for i in {1..6}; do echo \u0026#34;This is loop number $i\u0026#34; docker restart nginx sleep 30 done How to add my application log to Grafana 如何自訂application log 呢？ 從官網給的greenhouse 範例, 在in main_app.py, 我們看到\nlogging.basicConfig( format=\u0026#39;ts=%(asctime)s,%(msecs)03d level=%(levelname)s line=%(lineno)d msg=\u0026#34;%(message)s\u0026#34;\u0026#39;, datefmt=\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39; ) log = logging.getLogger(\u0026#39;werkzeug\u0026#39;) log.setLevel(logging.INFO) 所以其實寫到system log 即可\nConclusion This note demonstrated how to set up a log collection system using: Grafana Alloy, Loki and Grafana.\nhow to collect logs using Grafana Alloy and send to Loki how the collected logs been configured in Grafana. To sum up, the solution provides a way to redirect the log used to be generated at local to remote Grafana serve. That provides a unified interface.\nReference https://medium.com/@venkat65534/full-stack-observability-with-grafana-prometheus-loki-tempo-and-opentelemetry-90839113d17d\nhttps://github.com/grafana/loki-fundamentals\nhttps://grafana.com/docs/loki/latest/get-started/quick-start/tutorial/\nhttps://github.com/grafana/docker-monitor-workshop\nhttps://ithelp.ithome.com.tw/articles/10335935\nhttps://medium.com/@fawenyo/python-%E7%9B%A3%E6%8E%A7-prometheus-loki-grafana-25ead4bbb681\nhttps://medium.com/@derekjan1240/grafana-loki-prometheus-with-docker-compose-75d431bd07e2\n","permalink":"https://nikeasyanzi.github.io/en/posts/system-log-collection-with-grafana-loki-and-alloy/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eIn this article, we demonstrate how to effectively collecting logs with \u003ca href=\"https://github.com/grafana/alloy\"\u003eAlloy\u003c/a\u003e and \u003ca href=\"https://github.com/grafana/loki\"\u003eLoki\u003c/a\u003e.\nWith that, telemetry signals from applications, infrastructures are exported to on place for future telemetry.\u003c/p\u003e\n\u003cp\u003eIn the lab exercise, we try to setup a Nginx service along with Loki, Alloy, and observe the system log on Grafana.\u003c/p\u003e\n\u003ch1 id=\"configuration-deep-dive\"\u003eConfiguration deep dive\u003c/h1\u003e\n\u003ch2 id=\"grafana-alloy-configuration\"\u003eGrafana Alloy configuration\u003c/h2\u003e\n\u003cp\u003eA configuration file \u003ccode\u003econfig.alloy\u003c/code\u003e is used, that contains logs to collect and where to forward them to.\u003c/p\u003e","title":"System Log Collection With Grafana Loki and Alloy"},{"content":"Introduction Nowadays, in micro service architecture, engineers would host multiple containers to provide services, container monitor become a hot topic. In this post, we use cAdvisor to monitor all containers running on the host and visualize the data with Grafana.\nSetup Scripts for this experiment https://github.com/nikeasyanzi/Prometheus-Grafana-Cadvisor-Docker\nHere is the configuration for cadvisor in the docker compose file.\ncadvisor: container_name: cadvisor image: gcr.io/cadvisor/cadvisor:latest ports: - \u0026#34;8080:8080\u0026#34; privileged: true volumes: - \u0026#34;/:/rootfs:ro\u0026#34; - \u0026#34;/var/run:/var/run:rw\u0026#34; - \u0026#34;/var/lib/docker/:/var/lib/docker\u0026#34; - \u0026#34;/sys:/sys:ro\u0026#34; - \u0026#34;/dev/disk/:/dev/disk\u0026#34; devices: - \u0026#34;/dev/kmsg\u0026#34; # for kernel message Once all containers are up and running, We check cAdvisor by the link http://localhost:8080/containers/. We can see the list of all running containers\nAlso, scrolling the page down, we can see the CPU and memory usages of each container runtimes. Then we goto grafana, http://localhost:3000 We add a dashboard for the data visualization of the containers.\nfor cadvisor: https://grafana.com/grafana/dashboards/13946 For experiment purpose, k6 stress is used to generate traffics and see if the traffic is reflected on our system. Switching to the dashboard and at the bottom, we can see the RX/TX traffic coming in. Conclusion We successfully setting up a system for container monitoring. That includes 1. set up cAdvisor to monitor container metrics 2. integrated cAdvisor with Grafana for visualization\nAlso, we generate loading with k6 to validate the system capabilities.\nReference https://medium.com/@jsantoine24/monitoring-docker-containers-with-cadvisor-ed21a9cfae95 https://blog.darkthread.net/blog/cadvisor-prometheus-grafana/\nhttps://medium.com/@sohammohite/docker-container-monitoring-with-cadvisor-prometheus-and-grafana-using-docker-compose-b47ec78efbc\nhttps://blog.devops.dev/monitoring-with-cadvisor-prometheus-and-grafana-on-docker-8fc5c4a2eae7\nhttps://medium.com/@varunjain2108/monitoring-docker-containers-with-cadvisor-prometheus-and-grafana-d101b4dbbc84\n","permalink":"https://nikeasyanzi.github.io/en/posts/container-monitoring-with-cadvisor/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eNowadays, in micro service architecture, engineers would host multiple containers to provide services, container monitor become a hot topic. In this post, we use \u003ca href=\"https://github.com/google/cadvisor\"\u003ecAdvisor\u003c/a\u003e to monitor all containers running on the host and visualize the data with Grafana.\u003c/p\u003e\n\u003ch1 id=\"setup\"\u003eSetup\u003c/h1\u003e\n\u003cp\u003eScripts for this experiment\n\u003ca href=\"https://github.com/nikeasyanzi/Prometheus-Grafana-Cadvisor-Docker\"\u003ehttps://github.com/nikeasyanzi/Prometheus-Grafana-Cadvisor-Docker\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eHere is the configuration for cadvisor in the docker compose file.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003ecadvisor\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003econtainer_name\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ecadvisor\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eimage\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003egcr.io/cadvisor/cadvisor:latest\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eports\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    - \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;8080:8080\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eprivileged\u003c/span\u003e: \u003cspan style=\"color:#66d9ef\"\u003etrue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003evolumes\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    - \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/:/rootfs:ro\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    - \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/var/run:/var/run:rw\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    - \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/var/lib/docker/:/var/lib/docker\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    - \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/sys:/sys:ro\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    - \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/dev/disk/:/dev/disk\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003edevices\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    - \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/dev/kmsg\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e#  for kernel message\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eOnce all containers are up and running,\n\u003cimg loading=\"lazy\" src=\"https://i.imgur.com/d072x3l.png\"\u003e\u003c/p\u003e","title":"Container Monitoring With cAdvisor"},{"content":"Introduction In this article, we demonstrate\nhow to use Prometheus to collect and monitor our host machine and Nginx service. Visualization presentation of the collected metrics in Grafana Components Prometheus collects and stores its metrics as time series data, i.e. metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels. Prometheus node exporter exposes a wide variety of hardware- and kernel-related metrics. Grafana 一種圖形化處理軟體，可支援不同型式的來源資料數據，並以豐富的圖形來呈現相關數據 Configuration Update docker-compose services: nginx: image: nginx:latest container_name: nginx volumes: - ./nginx/:/etc/nginx/conf.d ports: - 8082:8080 nginx-prometheus-exporter: image: nginx/nginx-prometheus-exporter:1.4 container_name: nginx-prometheus-exporter command: -nginx.scrape-uri http://nginx:8080/stub_status ports: - 9113:9113 depends_on: - nginx # System monitoring node-exporter: container_name: node-exporter image: prom/node-exporter ports: - 9100:9100 prometheus: image: prom/prometheus:latest container_name: prometheus volumes: - ./prometheus.yaml:/etc/prometheus/prometheus.yaml - ./prometheus_data:/prometheus command: - \u0026#34;--config.file=/etc/prometheus/prometheus.yaml\u0026#34; ports: - \u0026#34;9090:9090\u0026#34; depends_on: - node-exporter - nginx-prometheus-exporter #handles rendering panels \u0026amp; dashboards to PNGs renderer: image: grafana/grafana-image-renderer:3.4.2 environment: BROWSER_TZ: Asia/Taipei ports: - \u0026#34;8081:8081\u0026#34; grafana: image: grafana/grafana:latest container_name: grafana volumes: - ./grafana_data:/var/lib/grafana environment: GF_SECURITY_ADMIN_PASSWORD: pass GF_RENDERING_SERVER_URL: http://renderer:8081/render GF_RENDERING_CALLBACK_URL: http://grafana:3000/ GF_LOG_FILTERS: rendering:debug depends_on: - prometheus ports: - \u0026#34;3000:3000\u0026#34; Update prometheus.yaml Configure Prometheus scrap interval\nglobal: scrape_interval: 5s # Server 抓取頻率 external_labels: monitor: \u0026#34;my-monitor\u0026#34; scrape_configs: - job_name: \u0026#34;prometheus\u0026#34; static_configs: - targets: [\u0026#34;localhost:9090\u0026#34;] - job_name: \u0026#34;node_exporter\u0026#34; static_configs: - targets: [\u0026#34;node-exporter:9100\u0026#34;] - job_name: \u0026#34;nginx_exporter\u0026#34; static_configs: - targets: [\u0026#34;nginx-prometheus-exporter:9113\u0026#34;] Start services Now, we use docker-compose to start our services.\nRun docker-compose.yaml docker-compose -f ./docker-compose.yaml up -d Check service status Service check and troubleshooting Next, we check the service status and make sure metrics are exported on the browser.\nNginx service status http://localhost:8082/\nCheck Nginx exported metrics http://localhost:8082/stub_status Nginx 狀態資訊 * Active connections: 2 目前有 2 個活動連線。\n* server accepts handled requests: 2 2 2 - **2**：已接受的連線數。 - **2**：已成功處理的連線數。 - **2**：總請求數（可能大於連線數，因為每個連線可以處理多個請求）。 * Reading: 0 Writing: 1 Waiting: 1 - Reading: 0：正在讀取請求的連線數。 - Writing: 1：正在傳送響應的連線數。 - Waiting: 1：空閒等待新請求的連線數（使用 HTTP Keep-Alive）。 P.S.當前伺服器負載較輕，共有 2 個連線，其中 1 個正在處理請求，1 個處於等待新請求的狀態。 Check metrics collected by nginx-prometheus-exporter http://localhost:9113/metrics Check host metrics collected by node exporter http://localhost:9100/metrics Check endpoint status at Prometheus Click http://localhost:9090/ to check if Prometheus is up and running. We can type up to query the current alive endpoints .\nWe can type target to query the current alive endpoints . http://localhost:9090/targets Now, the metrics are collect and redirected by Prometheus.\nVisualization output Next, we use Grafana to create visual data. Go to http://localhost:3000/ Select Data Source Click Test to test the connectivity between Prometheus and Grafana. Then, we go to Dashboards and select New -\u0026gt; New dashboard. We see the following page and select import dashboard to create new dashboard\n官網上也有很多dashboard template. 這邊我使用 Node exporter ID 1860 跟Nginx exporter ID 14900.\nDashboards\nfor nginx: https://grafana.com/grafana/dashboards/12708 for node exporter : https://grafana.com/grafana/dashboards/1860 We select load Also, select Prometheus data source and click import Once the dashboard is created, we can see the list of dashboards. Dashboard for Host Dashboard for Nginx Now, we have visualized data for monitor.\nTo generate traffic, we can use k6_script\ncat k6_script.js| docker run --rm -i grafana/k6 run - Conclusion In this article, we demonstrate\nUsing Prometheus to collect and monitor our host machine and Nginx service. The collected metric information is further visualized in Grafana Reference https://last9.hashnode.dev/how-to-download-and-run-node-exporter-using-docker#heading-step-2-run-the-node-exporter-container\nhttps://github.com/880831ian/Prometheus-Grafana-Docker\nhttps://mxulises.medium.com/simple-prometheus-setup-on-docker-compose-f702d5f98579\nhttps://github.com/880831ian/Prometheus-Grafana-Docker/tree/master\n","permalink":"https://nikeasyanzi.github.io/en/posts/visualized-monitoring-with-prometheus-grafana/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eIn this article, we demonstrate\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ehow to use Prometheus to collect and monitor our host machine and Nginx service.\u003c/li\u003e\n\u003cli\u003eVisualization presentation of the collected metrics in Grafana\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1 id=\"components\"\u003eComponents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePrometheus\u003c/strong\u003e collects and stores its metrics as time series data, i.e. metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePrometheus node exporter\u003c/strong\u003e exposes a wide variety of hardware- and kernel-related metrics.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrafana\u003c/strong\u003e 一種圖形化處理軟體，可支援不同型式的來源資料數據，並以豐富的圖形來呈現相關數據\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"configuration\"\u003eConfiguration\u003c/h1\u003e\n\u003ch2 id=\"update-docker-compose\"\u003eUpdate docker-compose\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eservices\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003enginx\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eimage\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003enginx:latest\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003econtainer_name\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003enginx\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003evolumes\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003e./nginx/:/etc/nginx/conf.d\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eports\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003e8082\u003c/span\u003e:\u003cspan style=\"color:#ae81ff\"\u003e8080\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003enginx-prometheus-exporter\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eimage\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003enginx/nginx-prometheus-exporter:1.4\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003econtainer_name\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003enginx-prometheus-exporter\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003ecommand\u003c/span\u003e: -\u003cspan style=\"color:#ae81ff\"\u003enginx.scrape-uri http://nginx:8080/stub_status\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eports\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003e9113\u003c/span\u003e:\u003cspan style=\"color:#ae81ff\"\u003e9113\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003edepends_on\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003enginx\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#75715e\"\u003e# System monitoring\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003enode-exporter\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003econtainer_name\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003enode-exporter\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eimage\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eprom/node-exporter\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eports\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003e9100\u003c/span\u003e:\u003cspan style=\"color:#ae81ff\"\u003e9100\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eprometheus\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eimage\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eprom/prometheus:latest\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003econtainer_name\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eprometheus\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003evolumes\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003e./prometheus.yaml:/etc/prometheus/prometheus.yaml\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003e./prometheus_data:/prometheus\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003ecommand\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;--config.file=/etc/prometheus/prometheus.yaml\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eports\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;9090:9090\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003edepends_on\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003enode-exporter\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003enginx-prometheus-exporter\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#75715e\"\u003e#handles rendering panels \u0026amp; dashboards to PNGs\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003erenderer\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eimage\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003egrafana/grafana-image-renderer:3.4.2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eenvironment\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003eBROWSER_TZ\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eAsia/Taipei\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eports\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;8081:8081\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003egrafana\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eimage\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003egrafana/grafana:latest\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003econtainer_name\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003egrafana\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003evolumes\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003e./grafana_data:/var/lib/grafana\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eenvironment\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003eGF_SECURITY_ADMIN_PASSWORD\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003eGF_RENDERING_SERVER_URL\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ehttp://renderer:8081/render\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003eGF_RENDERING_CALLBACK_URL\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ehttp://grafana:3000/\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003eGF_LOG_FILTERS\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003erendering:debug\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003edepends_on\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003eprometheus\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eports\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;3000:3000\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"update-prometheusyaml\"\u003eUpdate prometheus.yaml\u003c/h2\u003e\n\u003cp\u003eConfigure Prometheus scrap interval\u003c/p\u003e","title":"Visualized Monitoring With Prometheus and Grafana"},{"content":"Steps 1. Create Docker Compose File First, create a project directory to store n8n-related files. Run the following commands in the terminal:\nmkdir n8n-docker cd n8n-docker In the project directory, create a docker-compose.yml file to define the Docker containers for n8n and PostgreSQL.\ndocker-compose.yml\nservices: n8n: image: n8nio/n8n:nightly ports: - \u0026#34;5678:5678\u0026#34; environment: - DB_TYPE=postgresdb - DB_POSTGRESDB_HOST=postgres - DB_POSTGRESDB_DATABASE=${POSTGRES_DB} - DB_POSTGRESDB_USER=${POSTGRES_USER} - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD} volumes: - ./n8n_storage:/home/node/.n8n depends_on: - postgres postgres: image: postgres:13.22-alpine3.22 environment: - POSTGRES_USER=${POSTGRES_USER} - POSTGRES_PASSWORD=${POSTGRES_PASSWORD} - POSTGRES_DB=${POSTGRES_DB} volumes: - ./postgres_storage:/var/lib/postgresql/data volumes: n8n_storage: postgres_storage: Explanation:\nUses the n8n Docker image and maps port 5678 from the container to port 5678 on the host. postgres service: Uses the PostgreSQL Docker image and sets the username, password, and database name (stored in the .env file). volumes: Used to persist data for n8n and PostgreSQL respectively. GENERIC_TIMEZONE and TZ environment variables: Set the timezone to Taipei timezone. For more configuration details, refer to the n8n official documentation. 2. Create .env File In the project directory, create a .env file to set the PostgreSQL username, password, and database name. According to the Docker official documentation, the .env file will be treated as an environment variable file.\nPOSTGRES_USER=your_username POSTGRES_PASSWORD=your_password POSTGRES_DB=your_n8n_database 3. Start n8n Service Run the following command in the terminal to start the n8n and PostgreSQL services. You should see the created containers running in the background.\ndocker compose up -d We can check the PostgreSQL logs:\ndocker logs n8n-postgres-1 4. Open Browser and Access n8n Page http://localhost:5678 Reference https://darrenjon.com/docs/n8n/n8n-docker-setup/\n","permalink":"https://nikeasyanzi.github.io/en/posts/self-hosting-n8n-with-docker/","summary":"\u003ch1 id=\"steps\"\u003eSteps\u003c/h1\u003e\n\u003ch2 id=\"1-create-docker-compose-file\"\u003e1. Create Docker Compose File\u003c/h2\u003e\n\u003cp\u003eFirst, create a project directory to store n8n-related files. Run the following commands in the terminal:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003emkdir n8n-docker\ncd n8n-docker\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIn the project directory, create a \u003ccode\u003edocker-compose.yml\u003c/code\u003e file to define the Docker containers for n8n and PostgreSQL.\u003c/p\u003e\n\u003cp\u003edocker-compose.yml\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eservices\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003en8n\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eimage\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003en8nio/n8n:nightly\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eports\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;5678:5678\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eenvironment\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003eDB_TYPE=postgresdb\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003eDB_POSTGRESDB_HOST=postgres\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003eDB_POSTGRESDB_DATABASE=${POSTGRES_DB}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003eDB_POSTGRESDB_USER=${POSTGRES_USER}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003eDB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003evolumes\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003e./n8n_storage:/home/node/.n8n\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003edepends_on\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003epostgres\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003epostgres\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eimage\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003epostgres:13.22-alpine3.22\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eenvironment\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003ePOSTGRES_USER=${POSTGRES_USER}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003ePOSTGRES_PASSWORD=${POSTGRES_PASSWORD}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003ePOSTGRES_DB=${POSTGRES_DB}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003evolumes\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#ae81ff\"\u003e./postgres_storage:/var/lib/postgresql/data\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003evolumes\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003en8n_storage\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003epostgres_storage\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eExplanation:\u003c/p\u003e","title":"Self hosting local n8n service"},{"content":"Intro With ollama and Open WebUI, self-hosting a chatgpt like service is possible. It\u0026rsquo;s definitely a great news for people or corporations concerning about sharing their data with AI service providers. In this article, I will show you how to setup a service in local environment.\nSteps First of all, there are 2 services we need to setup, and we will host the service on container.\nPull ollama container image docker pull ollama/ollama:latest Before we run the service, we need to download a model from ollama model. For experiment, we select llama3.2. docker exec 5eeaf ollama pull llama3.2 Here, I select model llama3.2. It takes a while to download depends on the network and size of selected model.\nRun ollama service docker run -d -v $(PWD):/root/.ollama -p 11434:11434 --name ollama ollama/ollama Pull Open Web UI image docker pull ghcr.io/open-webui/open-webui:latest We create a folder named open-webui. Now, the directory lay out looks like the following. We see the downloaded llama3.2 model and the new created open-webui folder for Open Web UI data placement.. Run Open Web UI image docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v ./open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main It would takes a while to get the container ready to work. Once, the status shows healthy, we can try to open http://localhost:3000 For first time login, we will need to register a new account Once we log in with success, we can see the llama3.2 is selected and we can start to play with it. Docker compose with service I also write up a docker compose file, so we can host the file with one click. Check it out. https://github.com/nikeasyanzi/my-toolbox/tree/main/ollama_openwebui\n","permalink":"https://nikeasyanzi.github.io/en/posts/self-hosting-chatgpt-in-a-minute/","summary":"\u003ch1 id=\"intro\"\u003eIntro\u003c/h1\u003e\n\u003cp\u003eWith \u003ca href=\"https://ollama.com/\"\u003eollama\u003c/a\u003e and \u003ca href=\"https://github.com/open-webui/open-webui\"\u003eOpen WebUI\u003c/a\u003e, self-hosting a chatgpt like service is possible. It\u0026rsquo;s definitely a great news for people or corporations concerning about sharing their data with AI service providers.\nIn this article, I will show you how to setup a service in local environment.\u003c/p\u003e\n\u003ch1 id=\"steps\"\u003eSteps\u003c/h1\u003e\n\u003cp\u003eFirst of all, there are 2 services we need to setup, and we will host the service on container.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ePull \u003ca href=\"https://hub.docker.com/r/ollama/ollama\"\u003eollama container image\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-bash=\" data-lang=\"bash=\"\u003edocker pull ollama/ollama:latest\n\u003c/code\u003e\u003c/pre\u003e\u003col start=\"2\"\u003e\n\u003cli\u003eBefore we run the service, we need to download a model from \u003ca href=\"https://ollama.com/models\"\u003eollama model\u003c/a\u003e. For experiment, we select llama3.2.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i.imgur.com/iwluBge.png\"\u003e\u003c/p\u003e","title":"Self hosting a local AI in a minute with Ollama and Open WebUI"},{"content":"It\u0026rsquo;s been a headache to manage Python packages. But I feel I am saved from that when I meet UV.\nIntroduction There are two main concerns related to the Python package management problem.\nPython version: Different projects may need different Python interpreters. No one wants to mix them with the system default Python interpreters.\nThe packages/libraries needed by the project: Intuitively, different projects have different library dependencies. Even if they all rely on a popular library such as OpenSSL, they may depend on different library versions.\nSome tools are trying to solve the problem. For example,\nvenv and pip provide package management but are unable to switch between different versions of Python.\nPyenv solves the Python version switching problem but does not support package management.\nUV is developed and aimed at solving the aforementioned issues.\nHere, I walk through how to use uv to manage your project.\nWalkthrough Installation For macOS and Linux. I would use\ncurl -LsSf https://astral.sh/uv/install.sh | sh or through brew for macOS\nbrew install uv Important files The are some key files for UV to manage the project dependency. In addition, the good news is UV automatically generates and updates these files. Let\u0026rsquo;s take a quick look.\n.python-version: contains the Python version used for the project\npyproject.toml: serves as the main configuration file for project metadata and dependencies.\nuv.lock: Lock files for dependency management in UV.\nInitialization $ uv init my-uv Initialized project `my-uv` at `/Users/craigyang/workplace/my-uv` $ cd my-uv $ tree -a -L 1 . ├── .git ├── .gitignore ├── .python-version ├── main.py ├── pyproject.toml └── README.md 2 directories, 5 files Running Python scripts with UV Let\u0026rsquo;s take a look on the main.py\n$ cat main.py def main(): print(\u0026#34;Hello from my-uv!\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() While executing the main.py, a virtual environment is created automatically.\n$ uv run main.py Using CPython 3.13.4 Creating virtual environment at: .venv Hello from my-uv! Updating dependencies Here, we use requests as a new library to be added. We can see the content pyproject.toml is also updated.\n$ uv add requests Resolved 6 packages in 605ms Prepared 2 packages in 223ms Installed 5 packages in 10ms + certifi==2025.4.26 + charset-normalizer==3.4.2 + idna==3.10 + requests==2.32.4 + urllib3==2.4.0 $ cat pyproject.toml [project] name = \u0026#34;my-uv\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;Add your description here\u0026#34; readme = \u0026#34;README.md\u0026#34; requires-python = \u0026#34;\u0026gt;=3.13\u0026#34; dependencies = [ \u0026#34;requests\u0026gt;=2.32.4\u0026#34;, ] Managing Python Versions in UV In the following prompt, we see that for the my-uv project, Python3.13 is the default option, while in the system, it is installed with Python3.9.6\n$ uv python list cpython-3.14.0b1-macos-aarch64-none \u0026lt;download available\u0026gt; cpython-3.14.0b1+freethreaded-macos-aarch64-none \u0026lt;download available\u0026gt; cpython-3.13.4-macos-aarch64-none /opt/homebrew/bin/python3.13 -\u0026gt; ../Cellar/python@3.13/3.13.4/bin/python3.13 cpython-3.13.4-macos-aarch64-none /opt/homebrew/bin/python3 -\u0026gt; ../Cellar/python@3.13/3.13.4/bin/python3 cpython-3.13.4-macos-aarch64-none /Users/craigyang/.local/share/uv/python/cpython-3.13.4-macos-aarch64-none/bin/python3.13 cpython-3.13.4+freethreaded-macos-aarch64-none \u0026lt;download available\u0026gt; cpython-3.12.11-macos-aarch64-none \u0026lt;download available\u0026gt; cpython-3.11.13-macos-aarch64-none \u0026lt;download available\u0026gt; cpython-3.10.18-macos-aarch64-none \u0026lt;download available\u0026gt; cpython-3.9.23-macos-aarch64-none \u0026lt;download available\u0026gt; cpython-3.9.6-macos-aarch64-none /usr/bin/python3 Export requirements in UV uv export -o requirements.txt Conclusion We have shown how to use uv to start a new project. I also recommend a comprehensive article about uv for people interest in this topic.\n","permalink":"https://nikeasyanzi.github.io/en/posts/uv/","summary":"\u003cp\u003eIt\u0026rsquo;s been a headache to manage Python packages. But I feel I am saved from that when I meet \u003ca href=\"https://github.com/astral-sh/uv\"\u003eUV\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThere are two main concerns related to the Python package management problem.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ePython version: Different projects may need different Python interpreters. No one wants to mix them with the system default Python interpreters.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe packages/libraries needed by the project: Intuitively, different projects have different library dependencies. Even if they all rely on a popular library such as OpenSSL, they may depend on different library versions.\u003c/p\u003e","title":"UV: A Python Package Management Tool"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code Code Blocks Inline Code This is Inline Code\nOnly pre Code block with backticks \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with backticks and language specified 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nikeasyanzi.github.io/en/posts/markdown-syntax/","summary":"\u003cp\u003eThis article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\u003c/p\u003e","title":"Markdown Syntax Guide"},{"content":"I am a software engineer from Taiwan. Mainly focus on Linux, and Self hosting.\n","permalink":"https://nikeasyanzi.github.io/en/about/","summary":"\u003cp\u003eI am a software engineer from Taiwan. Mainly focus on Linux, and Self hosting.\u003c/p\u003e","title":"About Me"}]