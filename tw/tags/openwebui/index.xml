<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>OpenWebUI on Craig Yang&#39;s Blog</title>
    <link>https://nikeasyanzi.github.io/tw/tags/openwebui/</link>
    <description>Recent content in OpenWebUI on Craig Yang&#39;s Blog</description>
    <generator>Hugo -- 0.147.9</generator>
    <language>tw</language>
    <copyright>PaperMod Contributors</copyright>
    <lastBuildDate>Tue, 22 Jul 2025 16:10:56 +0800</lastBuildDate>
    <atom:link href="https://nikeasyanzi.github.io/tw/tags/openwebui/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>使用 Ollama 和 Open WebUI 在一分鐘內自架本地 AI</title>
      <link>https://nikeasyanzi.github.io/tw/posts/self-hosting-chatgpt-in-a-minute/</link>
      <pubDate>Tue, 22 Jul 2025 16:10:56 +0800</pubDate>
      <guid>https://nikeasyanzi.github.io/tw/posts/self-hosting-chatgpt-in-a-minute/</guid>
      <description>&lt;h1 id=&#34;簡介-intro&#34;&gt;簡介 (Intro)&lt;/h1&gt;
&lt;p&gt;透過 &lt;a href=&#34;https://ollama.com/&#34;&gt;ollama&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/open-webui/open-webui&#34;&gt;Open WebUI&lt;/a&gt;，自架類似 ChatGPT 的服務已成為可能。對於關注與 AI 服務供應商分享資料問題的個人或企業來說，這絕對是個好消息。
在本文中，我將展示如何在本地環境中設置此服務。&lt;/p&gt;
&lt;h1 id=&#34;步驟-steps&#34;&gt;步驟 (Steps)&lt;/h1&gt;
&lt;p&gt;首先，我們需要設置 2 個服務，並將這些服務架設在容器上。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;拉取 &lt;a href=&#34;https://hub.docker.com/r/ollama/ollama&#34;&gt;ollama container image&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-bash=&#34; data-lang=&#34;bash=&#34;&gt;docker pull ollama/ollama:latest
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;在執行服務之前，我們需要從 &lt;a href=&#34;https://ollama.com/models&#34;&gt;ollama model&lt;/a&gt; 下載模型。作為實驗，我們選擇 llama3.2。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://i.imgur.com/iwluBge.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://i.imgur.com/lnBbjDQ.png&#34;&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-bash=&#34; data-lang=&#34;bash=&#34;&gt;docker exec 5eeaf ollama pull llama3.2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;這裡我選擇了 llama3.2 模型。下載需要一些時間，取決於網路速度和所選模型的大小。
&lt;img loading=&#34;lazy&#34; src=&#34;https://i.imgur.com/DfxrHZv.png&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;執行 ollama 服務&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-bash=&#34; data-lang=&#34;bash=&#34;&gt;docker run -d -v $(PWD):/root/.ollama -p 11434:11434 --name ollama ollama/ollama
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;拉取 Open Web UI 映像檔&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-bash=&#34; data-lang=&#34;bash=&#34;&gt;docker pull ghcr.io/open-webui/open-webui:latest
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;我們建立一個名為 open-webui 的資料夾。現在，目錄結構如下所示。
我們可以看到已下載的 llama3.2 模型以及新建立的 open-webui 資料夾，用於存放 Open Web UI 資料。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://i.imgur.com/OBRCjpR.png&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
